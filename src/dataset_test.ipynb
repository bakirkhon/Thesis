{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2d37a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4779/3816426850.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  graph['X'] = torch.tensor(graph['X'], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9300, 0.0404, 0.0295])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from pathlib import Path\n",
    "from src.datasets.spectre_dataset import SpectreGraphDataModule, SpectreDatasetInfos\n",
    "from src.datasets.abstract_dataset import AbstractDataModule, AbstractDatasetInfos\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Load training dataset\n",
    "file_path='/home/bakirkhon/DiGress/training_dataset/training_dataset.pt'\n",
    "raw_path='/home/bakirkhon/DiGress/training_dataset/raw'\n",
    "processed_path='/home/bakirkhon/DiGress/training_dataset/processed'\n",
    "all_graphs=torch.load(file_path)\n",
    "\n",
    "# Split\n",
    "num_graphs=len(all_graphs)\n",
    "test_len=int(round(num_graphs*0.2))\n",
    "train_len=int(round(num_graphs-test_len)*0.8)\n",
    "val_len=num_graphs-test_len-train_len\n",
    "\n",
    "g_cpu=torch.Generator()\n",
    "g_cpu.manual_seed(0)\n",
    "\n",
    "indices=torch.randperm(num_graphs,generator=g_cpu)\n",
    "train_indices=indices[:train_len]\n",
    "val_indices=indices[train_len:train_len+val_len]\n",
    "test_indices=indices[train_len+val_len:]\n",
    "\n",
    "train_data=[]\n",
    "val_data=[]\n",
    "test_data=[]\n",
    "\n",
    "for i, graph in enumerate(all_graphs):\n",
    "    graph['X'] = torch.tensor(graph['X'], dtype=torch.float)\n",
    "    graph['E'] = torch.tensor(graph['E'], dtype=torch.float)\n",
    "    if i in train_indices:\n",
    "        train_data.append(graph)\n",
    "    elif i in val_indices:\n",
    "        val_data.append(graph)\n",
    "    elif i in test_indices:\n",
    "        test_data.append(graph)\n",
    "    else:\n",
    "        raise ValueError(f'Index {i} not in any split')\n",
    "\n",
    "torch.save(train_data, os.path.join(raw_path, 'train.pt'))\n",
    "torch.save(val_data, os.path.join(raw_path, 'val.pt'))\n",
    "torch.save(test_data, os.path.join(raw_path,'test.pt'))\n",
    "\n",
    "# Process\n",
    "def process(dataset: str):\n",
    "    raw_dataset=torch.load(os.path.join(raw_path,f'{dataset}.pt'))\n",
    "    \n",
    "    data_list=[]\n",
    "    for graph in raw_dataset:\n",
    "        X=graph['X']\n",
    "        E=graph['E']\n",
    "        n=X.shape[0]\n",
    "        # first row=source nodes, second row=destination rows\n",
    "        edge_index, _=dense_to_sparse((E.sum(-1)>0).float())\n",
    "        edge_attr=E[edge_index[0],edge_index[1],:]\n",
    "        num_nodes=n*torch.ones(1,dtype=torch.long)\n",
    "        data=Data(x=X,edge_index=edge_index,edge_attr=edge_attr,n_nodes=num_nodes)\n",
    "        data_list.append(data)\n",
    "    torch.save(InMemoryDataset.collate(data_list),os.path.join(processed_path,f'{dataset}.pt'))\n",
    "\n",
    "train_processed=process('train')\n",
    "val_processed=process('val')\n",
    "test_processed=process('test')\n",
    "\n",
    "# Computes the empirical distribution of graph sizes (number of nodes) \n",
    "def node_counts(dataset: str, max_nodes_possible=50):\n",
    "    processed_data,slices=torch.load(os.path.join(processed_path,f'{dataset}.pt'))\n",
    "    # Wrap into a dataset\n",
    "    class DummyDataset(InMemoryDataset):\n",
    "        def __init__(self, data, slices):\n",
    "            super().__init__()\n",
    "            self.data=data\n",
    "            self.slices = slices\n",
    "\n",
    "    dataset=DummyDataset(processed_data, slices)\n",
    "    loader=DataLoader(dataset, batch_size=512)\n",
    "    all_counts=torch.zeros(max_nodes_possible)\n",
    "    for data in loader:\n",
    "        unique, counts=torch.unique(data.batch,return_counts=True)\n",
    "        for count in counts:\n",
    "            all_counts[count]+=1\n",
    "    max_index=max(all_counts.nonzero())\n",
    "    all_counts=all_counts[:max_index+1]\n",
    "    all_counts=all_counts/all_counts.sum()\n",
    "    return all_counts\n",
    "\n",
    "train_nodes=node_counts('train')\n",
    "val_nodes=node_counts('val')\n",
    "\n",
    "# def node_types(dataset: str):\n",
    "#     num_classes=None\n",
    "#     processed_data,slices=torch.load(os.path.join(processed_path,f'{dataset}.pt'))\n",
    "#     # Wrap into a dataset\n",
    "#     class DummyDataset(InMemoryDataset):\n",
    "#         def __init__(self, data, slices):\n",
    "#             super().__init__()\n",
    "#             self.data=data\n",
    "#             self.slices = slices\n",
    "\n",
    "#     dataset=DummyDataset(processed_data, slices)\n",
    "#     loader=DataLoader(dataset, batch_size=512)\n",
    "#     for data in loader:\n",
    "#         num_classes=data.x.shape[1]\n",
    "#         break\n",
    "\n",
    "#     counts=torch.zeros(num_classes)\n",
    "\n",
    "#     for i, data in enumerate(loader):\n",
    "#         counts+=data.x.sum(dim=0)\n",
    "    \n",
    "#     counts=counts/counts.sum()\n",
    "#     return counts\n",
    "\n",
    "# train_node_types=node_types('train')\n",
    "\n",
    "def edge_counts(dataset: str):\n",
    "    num_classes=None\n",
    "    processed_data,slices=torch.load(os.path.join(processed_path,f'{dataset}.pt'))\n",
    "    # Wrap into a dataset\n",
    "    class DummyDataset(InMemoryDataset):\n",
    "        def __init__(self, data, slices):\n",
    "            super().__init__()\n",
    "            self.data=data\n",
    "            self.slices = slices\n",
    "    dataset=DummyDataset(processed_data, slices)\n",
    "    loader=DataLoader(dataset, batch_size=512)\n",
    "    for data in loader:\n",
    "        num_classes=data.edge_attr.shape[1]\n",
    "        break\n",
    "    \n",
    "    d=torch.zeros(num_classes,dtype=torch.float)\n",
    "\n",
    "    for i,data in enumerate(loader):\n",
    "        uniqie, counts=torch.unique(data.batch,return_counts=True)\n",
    "\n",
    "        all_pairs=0\n",
    "        for count in counts:\n",
    "            all_pairs+=count*(count-1)\n",
    "\n",
    "        num_edges=data.edge_index.shape[1]\n",
    "        num_non_edges=all_pairs-num_edges\n",
    "        \n",
    "        edge_types=data.edge_attr.sum(dim=0)\n",
    "        assert num_non_edges>=0\n",
    "        d[0]+=num_non_edges\n",
    "        d[1:]+=edge_types[1:]\n",
    "    \n",
    "    d=d/d.sum()\n",
    "    print(d)\n",
    "\n",
    "train_edges=edge_counts('train')\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digress",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
